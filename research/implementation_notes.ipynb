{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Transformer\n",
    "\n",
    "> Based on the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "> Google's original implementation in TensorFlow [here](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)\n",
    "\n",
    "The Transformer network is an attention only sequence transduction model \"dispensing with recurrence and convolutions entirely\".\n",
    "\n",
    "# Transformer Architecture \n",
    "\n",
    "The model is made up of $N$ identical encoder/decoder layers ($N=6$ in the paper).\n",
    "\n",
    "![Transformer Model Architecture](./images/network_architecture.png)\n",
    "\n",
    "Each encoder/decoder is made up of two types of Sub-Layers:\n",
    "\n",
    "- A Multi-Head Attention Layer\n",
    "- A Feed-Forward Network\n",
    "\n",
    "**TODO**: Layer-normilization is applied around each sub-layer before being passed to the next. Don't forget to implement this in the full model later.\n",
    "\n",
    "We're going to implement from more or less scratch! So we need to dig down into the sub-layers and build up.\n",
    "\n",
    "## Sub-Layer: Multi-Head Attention\n",
    "\n",
    "> An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
    "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    ">-- Attention is All You Need, 3.2\n",
    "\n",
    "These queries, keys and values are computed as matrices\n",
    "\n",
    "The sub-layer looks like:\n",
    "\n",
    "![Multi-head attention](./images/multi-head.png)\n",
    "\n",
    "Where:\n",
    "\n",
    "$$MultiHead(Q, K, V) = Concat(head_{1}, \\ldots{}, head_{h}) W^{O}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$head_i = Attention(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})$$\n",
    "\n",
    "\n",
    "### Implementation: Scaled Attention Function\n",
    "\n",
    "Recursing ever further into our model, we need the math behind a single Attention calculation:\n",
    "\n",
    "![Attention Function](./images/attention_calc.png)\n",
    "\n",
    "or, in notation:\n",
    "\n",
    "$$Attention(Q,K,V) = softmax \\left( \\frac{ QK^{T} }{ \\sqrt{ d_{k} } } \\right) V$$\n",
    "\n",
    "This is a version of multiplicative attention. It's scaled by the dimension of the key to prevent the dot product from getting out of hand (see this [blog post](http://ruder.io/deep-learning-nlp-best-practices/index.html#attention) for a nice overview of additive vs. multiplicative attention)\n",
    "\n",
    "\n",
    "Some notes about the google implementation:\n",
    "- The optional mask is implemented simply by setting masked values to $-\\infty$.\n",
    "- Later in the paper they add dropout to the result of the softmax. We'll just put it in now, as they found it improved the model\n",
    "\n",
    "Let's do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_key, drop_percent=0.1):\n",
    "        super(ScaledAttention, self).__init__()\n",
    "        \n",
    "        # The value to scale by will be constant throughout model\n",
    "        self.scale_value = np.sqrt(dim_key)\n",
    "        \n",
    "        # Layers\n",
    "        self.dropout = nn.Dropout(drop_percent)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \n",
    "        # Remember, the 1st dim of torch tensors is the batch size\n",
    "        attention = torch.bmm(Q, K.transpose(1, 2))\n",
    "        \n",
    "        if mask:\n",
    "            attention.masked_fill_(mask, -float('inf'))\n",
    "            \n",
    "        # Ugh... python... get you a pipe operator or something\n",
    "        attention = self.droupout(self.softmax(attention))\n",
    "        \n",
    "        attention = torch.bmm(attention, V)\n",
    "        \n",
    "        return attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Multi-Head Attention\n",
    "\n",
    "Now we have what we need for the Multi-Head sub-layer itself. From the paper:\n",
    "\n",
    "> Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_q$, $d_k$ and $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values ...\n",
    "\n",
    "> Attention is All You Need, 3.2.2\n",
    "\n",
    "\n",
    "Relevant formulas from above:\n",
    "$$MultiHead(Q, K, V) = Concat(head_{1}, \\ldots{}, head_{h}) W^{O}$$\n",
    "\n",
    "$$head_i = Attention(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})$$\n",
    "\n",
    "W/r/t dimensions of stuff: \n",
    "\n",
    "$W_{i}^{Q} \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_{i}^{K} \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_{i}^{V} \\in \\mathbb{R}^{d_{model} \\times d_v}$, and $W_{i}^{O} \\in \\mathbb{R}^{d_{hd_v} \\times d_{model}}$\n",
    "\n",
    "where in the paper: \n",
    "- $h = 8$\n",
    "- $d_{model} = 512$\n",
    "- $d_q = d_k = d_v = d_{model}/h = 64$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, dim_model, dim_key, dim_value, drop_percent=0.1):\n",
    "        super(MultiHeadAttenion, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.dim_key   = dim_key\n",
    "        self.dim_value = dim_value\n",
    "        \n",
    "        # Projecting w/ learned weight matrix is just a linear transform w/out bias, yes?\n",
    "        # TODO: Will these Linear units learn num_heads seperate weights \n",
    "        #       if an <num_heads> x <dim_model> Tensor is passed in? \n",
    "        #       is passed through? I don't actually know. \n",
    "        #\n",
    "        #       Would need to do the projection \"by hand\" if not (i.e. with our own weight Variables)\n",
    "        q_projection = nn.Linear(dim_model, dim_key, bias=False)\n",
    "        k_projection = nn.Linear(dim_model, dim_key, bias=False)\n",
    "        v_projection = nn.Linear(dim_model, dim_value, bias=False)\n",
    "        \n",
    "        o_projection = nn.Linear(num_heads*dim_value, dim_model, bias=False)\n",
    "        \n",
    "        # Layers\n",
    "        self.scaled_attention = ScaledAttention(dim_key, drop_percent)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        # If I understand PyTorch, we'll be getting in Tensors of shape\n",
    "        # <Minibatch Size> x <Length Of Vec> x <Dimmension of Embedding Space>\n",
    "        # where dim(embedding) would be dim(model)\n",
    "        mini_batch_size, len_q, dim_model = q.size()\n",
    "        _,               len_k, _         = k.size()\n",
    "        _,               len_v, _         = v.size()\n",
    "        \n",
    "\n",
    "        # We want to duplicate the vectors num_heads times (each will be projected into dim_key/value),\n",
    "        # then reshape into <num_heads> x <len_vec * mini_batch_size ?> x <dim_model>  \n",
    "        Q = q.repeat(self.num_heads, 1, 1).view(self.num_heads, -1, self.dim_model)\n",
    "        K = k.repeat(self.num_heads, 1, 1).view(self.num_heads, -1, self.dim_model)\n",
    "        V = v.repeat(self.num_heads, 1, 1).view(self.num_heads, -1, self.dim_model)\n",
    "        \n",
    "        # Now pass through the linear projection and reshape to \n",
    "        # some larger batch size of <length of vector> x <dim_key/value> (which our attention function expects)\n",
    "        Q = q_projection(Q).shape(-1, len_q, self.dim_key)\n",
    "        K = k_projection(K).shape(-1, len_k, self.dim_key)\n",
    "        V = v_projection(V).shape(-1, len_v, self.dim_value)\n",
    "        \n",
    "        # note, if we make our mask based on a single <vec> x <embedding>\n",
    "        # we need a tensor of num_heads identical masks I think maybe?\n",
    "        if mask:\n",
    "            mask = mask.repeat(self.num_heads, 1, 1)\n",
    "            \n",
    "        # Pass our matrices to our Attention layer\n",
    "        attention = scaled_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Chunk so first dimension is the expected mini batch size using split()\n",
    "        # Then concat along the last axis to get expected size to match with dimensions of W^O\n",
    "        attention = torch.cat(seq = torch.split(attention, mini_batch_size, dim=0), \n",
    "                              dim = -1)\n",
    "        \n",
    "        # Project back to tensor of <mini_batch> x <dim_model> x <dim_model>\n",
    "        attention = o_projection(attention)\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
