{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Transformer\n",
    "\n",
    "> Based on the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "> Google's original implementation in TensorFlow [here](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)\n",
    "\n",
    "The Transformer network is an attention only sequence transduction model \"dispensing with recurrence and convolutions entirely\".\n",
    "\n",
    "# Transformer Architecture \n",
    "\n",
    "The model is made up of $N$ identical encoder/decoder layers ($N=6$ in the paper).\n",
    "\n",
    "![Transformer Model Architecture](./images/network_architecture.png)\n",
    "\n",
    "Each encoder/decoder is made up of two types of Sub-Layers:\n",
    "\n",
    "- A Multi-Head Attention Layer\n",
    "- A Feed-Forward Network\n",
    "\n",
    "**TODO**: Layer-normilization is applied around each sub-layer before being passed to the next. Don't forget to implement this in the full model later.\n",
    "\n",
    "We're going to implement from more or less scratch! So we need to dig down into the sub-layers and build up.\n",
    "\n",
    "## Sub-Layer: Multi-Head Attention\n",
    "\n",
    "> An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
    "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    ">-- Attention is All You Need, 3.2\n",
    "\n",
    "These queries, keys and values are computed as matrices\n",
    "\n",
    "The sub-layer looks like:\n",
    "\n",
    "![Multi-head attention](./images/multi-head.png)\n",
    "\n",
    "Where:\n",
    "\n",
    "$$MultiHead(Q, K, V) = Concat(head_{1}, \\ldots{}, head_{h}) W^{O}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$head_i = Attention(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})$$\n",
    "\n",
    "\n",
    "### Single Attention Computation\n",
    "\n",
    "Recursing ever further into our model, we need the math behind a single Attention calculation:\n",
    "\n",
    "![Attention Function](./images/attention_calc.png)\n",
    "\n",
    "or, in notation:\n",
    "\n",
    "$$Attention(Q,K,V) = softmax \\left( \\frac{ QK^{T} }{ \\sqrt{ d_{k} } } \\right) V$$\n",
    "\n",
    "This is a version of multiplicative attention. It's scaled by the dimension of the key to prevent the dot product from getting out of hand (see this [blog post](http://ruder.io/deep-learning-nlp-best-practices/index.html#attention) for a nice overview of additive vs. multiplicative attention)\n",
    "\n",
    "\n",
    "Some notes about the google implementation:\n",
    "- The optional mask is implemented simply by setting masked values to $-\\infty$.\n",
    "- Later in the paper\n",
    "\n",
    "This is something we can implement! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledMultiAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_key, drop_percent=0.1):\n",
    "        \n",
    "        super(ScaledMultiAttention, self).__init__()\n",
    "        \n",
    "        # The value to scale by will be constant throughout model\n",
    "        self.scale_value = np.sqrt(dim_key)\n",
    "        \n",
    "        # Layers\n",
    "        self.dropout = nn.Dropout(drop_percent)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \n",
    "        # Remember, the 1st dim of torch tensors is the batch size\n",
    "        attention = torch.bmm(Q, K.transpose(1, 2))\n",
    "        \n",
    "        if mask:\n",
    "            attention.masked_fill_(mask, -float('inf'))\n",
    "            \n",
    "        # Ugh... python... get you a pipe operator or something\n",
    "        attention = self.droupout(self.softmax(attention))\n",
    "        \n",
    "        attention = torch.bmm(attention, V)\n",
    "        \n",
    "        return attention\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
